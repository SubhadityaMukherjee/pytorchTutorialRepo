{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "according-chancellor",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torchsnooper as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from MovingMNIST import MovingMNIST\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "linear-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsnooper as tsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-updating",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# @tsp.snoop()\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (\n",
    "            torch.zeros(\n",
    "                batch_size,\n",
    "                self.hidden_dim,\n",
    "                height,\n",
    "                width,\n",
    "                device=self.conv.weight.device,\n",
    "            ),\n",
    "            torch.zeros(\n",
    "                batch_size,\n",
    "                self.hidden_dim,\n",
    "                height,\n",
    "                width,\n",
    "                device=self.conv.weight.device,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stuffed-regulation",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# @tsp.snoop()\n",
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, nf, in_chan):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "        \"\"\" ARCHITECTURE \n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoder Vector (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoder Vector as input\n",
    "        # Decoder (3D CNN) - produces regression predictions for our model\n",
    "        \"\"\"\n",
    "        self.encoder_1_convlstm = ConvLSTMCell(\n",
    "            input_dim=in_chan, hidden_dim=nf, kernel_size=(3, 3), bias=True\n",
    "        )\n",
    "\n",
    "        self.encoder_2_convlstm = ConvLSTMCell(\n",
    "            input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True\n",
    "        )\n",
    "\n",
    "        self.decoder_1_convlstm = ConvLSTMCell(\n",
    "            input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True  # nf + 1\n",
    "        )\n",
    "\n",
    "        self.decoder_2_convlstm = ConvLSTMCell(\n",
    "            input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True\n",
    "        )\n",
    "\n",
    "        self.decoder_CNN = nn.Conv3d(\n",
    "            in_channels=nf, out_channels=1, kernel_size=(1, 3, 3), padding=(0, 1, 1)\n",
    "        )\n",
    "\n",
    "    def autoencoder(\n",
    "        self, x, seq_len, future_step, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4\n",
    "    ):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1_convlstm(\n",
    "                input_tensor=x[:, t, :, :], cur_state=[h_t, c_t]\n",
    "            )  # we could concat to provide skip conn here\n",
    "            h_t2, c_t2 = self.encoder_2_convlstm(\n",
    "                input_tensor=h_t, cur_state=[h_t2, c_t2]\n",
    "            )  # we could concat to provide skip conn here\n",
    "\n",
    "        encoder_vector = h_t2\n",
    "\n",
    "        for t in range(future_step):\n",
    "            h_t3, c_t3 = self.decoder_1_convlstm(\n",
    "                input_tensor=encoder_vector, cur_state=[h_t3, c_t3]\n",
    "            )  # we could concat to provide skip conn here\n",
    "            h_t4, c_t4 = self.decoder_2_convlstm(\n",
    "                input_tensor=h_t3, cur_state=[h_t4, c_t4]\n",
    "            )  # we could concat to provide skip conn here\n",
    "            encoder_vector = h_t4\n",
    "            outputs += [h_t4]\n",
    "\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = self.decoder_CNN(outputs)\n",
    "        outputs = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, future_seq=0, hidden_state=None):\n",
    "\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "\n",
    "        h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(\n",
    "            batch_size=b, image_size=(h, w)\n",
    "        )\n",
    "        h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(\n",
    "            batch_size=b, image_size=(h, w)\n",
    "        )\n",
    "        h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(\n",
    "            batch_size=b, image_size=(h, w)\n",
    "        )\n",
    "\n",
    "        outputs = self.autoencoder(\n",
    "            x, seq_len, future_seq, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4\n",
    "        )\n",
    "\n",
    "        #         print(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "approved-brown",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# @tsp.snoop()\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden_dims,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.0001,\n",
    "        hparams=None,\n",
    "        batch_size=64,\n",
    "    ):\n",
    "        super(LitModel, self).__init__()\n",
    "\n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps_past = 10\n",
    "        self.n_steps_ahead = 10\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.conv_lstm_model = EncoderDecoderConvLSTM(nf=self.n_hidden_dims, in_chan=1)\n",
    "\n",
    "    def create_video(self, x, y_hat, y):\n",
    "\n",
    "        preds = torch.cat([x.cpu(), y_hat.unsqueeze(2).cpu()], dim=1)[0]\n",
    "\n",
    "        y_plot = torch.cat([x.cpu(), y.unsqueeze(2).cpu()], dim=1)[0]\n",
    "\n",
    "        difference = (torch.pow(y_hat[0] - y[0], 2)).detach().cpu()\n",
    "        zeros = torch.zeros(difference.shape)\n",
    "        difference_plot = torch.cat(\n",
    "            [zeros.cpu().unsqueeze(0), difference.unsqueeze(0).cpu()], dim=1\n",
    "        )[0].unsqueeze(1)\n",
    "\n",
    "        final_image = torch.cat([preds, y_plot, difference_plot], dim=0)\n",
    "\n",
    "        grid = torchvision.utils.make_grid(\n",
    "            final_image, nrow=self.n_steps_past + self.n_steps_ahead\n",
    "        )\n",
    "\n",
    "        return grid\n",
    "\n",
    "    #     @tsp.snoop()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_lstm_model(x, future_seq=self.n_steps_ahead)\n",
    "        return output\n",
    "\n",
    "    #     @tsp.snoop()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #         print(list(self.parameters())\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        return ([optimizer], [scheduler])\n",
    "\n",
    "    #     @tsp.snoop()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = (\n",
    "            batch[:, 0 : self.n_steps_past, :, :, :],\n",
    "            batch[:, self.n_steps_past :, :, :, :],\n",
    "        )\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        y = y.squeeze()\n",
    "        preds = self(x).squeeze()\n",
    "        loss = self.criterion(preds, y)\n",
    "        lr_saved = self.trainer.optimizers[0].param_groups[-1][\"lr\"]\n",
    "        lr_saved = torch.scalar_tensor(lr_saved).cuda()\n",
    "        final_image = self.create_video(x, preds, y)\n",
    "        #         plt.plot(final_image)\n",
    "        #         plt.show()\n",
    "        #         plt.savefig(f\"./{batch_idx}_im.jpg\")\n",
    "        self.logger.experiment.add_image(\n",
    "            \"epoch_\"\n",
    "            + str(self.current_epoch)\n",
    "            + \"_step\"\n",
    "            + str(self.global_step)\n",
    "            + \"_generated_images\",\n",
    "            final_image,\n",
    "            0,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"lr_saved\", self.learning_rate)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = (\n",
    "            batch[:, 0 : self.n_steps_past, :, :, :],\n",
    "            batch[:, self.n_steps_past :, :, :, :],\n",
    "        )\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        y = y.squeeze()\n",
    "        preds = self(x).squeeze()\n",
    "        loss = self.criterion(preds, y)\n",
    "        lr_saved = self.trainer.optimizers[0].param_groups[-1][\"lr\"]\n",
    "        lr_saved = torch.scalar_tensor(lr_saved).cuda()\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"validation_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"mean_val_loss\", avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-fence",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flying-verse",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class ImDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        data_dir: str = \"/media/hdd/Datasets/movingMNIST/\",\n",
    "        img_size=(256, 256),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps_past = 10\n",
    "        self.n_steps_ahead = 10\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        self.train_dataset = MovingMNIST(\n",
    "            train=True,\n",
    "            data_root=self.data_dir,\n",
    "            seq_len=self.n_steps_past + self.n_steps_ahead,\n",
    "            image_size=64,\n",
    "            num_digits=2,\n",
    "        )\n",
    "\n",
    "        self.valid_dataset = MovingMNIST(\n",
    "            train=False,\n",
    "            data_root=self.data_dir,\n",
    "            seq_len=self.n_steps_past + self.n_steps_ahead,\n",
    "            image_size=64,\n",
    "            num_digits=2,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, num_workers=12, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset, batch_size=self.batch_size, num_workers=12\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intellectual-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "img_size = 128\n",
    "n_hidden_dims = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tamil-mount",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eragon/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:279: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "dm = ImDataModule(batch_size=batch_size, img_size=img_size)\n",
    "class_ids = dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "substantial-tractor",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# # Logs\n",
    "\n",
    "model = LitModel(batch_size=batch_size, n_hidden_dims=n_hidden_dims)\n",
    "# logger = CSVLogger(\"logs\", name=\"lstmEncDec\")\n",
    "logger = TensorBoardLogger(save_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suffering-racing",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/eragon/.local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Disable automatic optimization with the trainer flag is deprecated and will be removed in v1.3.0!Please use the property on the LightningModule for disabling automatic optimization\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Using native 16bit precision.\n",
      "/home/eragon/.local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: plugin <class 'pytorch_lightning.plugins.sharded_plugin.DDPShardedPlugin'> has added additional required plugins as default: [<class 'pytorch_lightning.plugins.sharded_native_amp_plugin.ShardedNativeAMPPlugin'>]Extend this plugin and override `required_plugins`if this conflicts with your additional plugins.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    auto_select_gpus=True,\n",
    "    gpus=1,\n",
    "    precision=16,\n",
    "    profiler=True,\n",
    "    max_epochs=1,\n",
    "    callbacks=[pl.callbacks.ProgressBar()],\n",
    "    automatic_optimization=True,\n",
    "    enable_pl_optimizer=True,\n",
    "    accumulate_grad_batches=16,\n",
    "    logger=logger,\n",
    "    accelerator=\"ddp\",\n",
    "    plugins=\"ddp_sharded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "christian-maker",
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | criterion       | MSELoss                | 0     \n",
      "1 | conv_lstm_model | EncoderDecoderConvLSTM | 1.0 M \n",
      "-----------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da77f145dc2409e8e3ed96e85f050e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eragon/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                    \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "Total                     \t|  -              \t|_              \t|  1838.3         \t|  100 %          \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch        \t|  1837.3         \t|1              \t|  1837.3         \t|  99.948         \t|\n",
      "run_training_batch        \t|  0.57796        \t|3000           \t|  1733.9         \t|  94.322         \t|\n",
      "training_step_and_backward\t|  0.31919        \t|3000           \t|  957.57         \t|  52.091         \t|\n",
      "model_forward             \t|  0.20641        \t|3000           \t|  619.24         \t|  33.686         \t|\n",
      "model_backward            \t|  0.11247        \t|3000           \t|  337.42         \t|  18.355         \t|\n",
      "evaluation_step_and_end   \t|  0.1835         \t|502            \t|  92.118         \t|  5.0112         \t|\n",
      "closure                   \t|  0.32263        \t|188            \t|  60.654         \t|  3.2995         \t|\n",
      "optimizer_step            \t|  0.2564         \t|188            \t|  48.204         \t|  2.6223         \t|\n",
      "on_train_batch_end        \t|  0.0012178      \t|3000           \t|  3.6534         \t|  0.19874        \t|\n",
      "get_train_batch           \t|  0.0010854      \t|3000           \t|  3.2562         \t|  0.17713        \t|\n",
      "on_validation_batch_end   \t|  0.0012106      \t|502            \t|  0.60774        \t|  0.033061       \t|\n",
      "cache_result              \t|  2.6355e-05     \t|13710          \t|  0.36133        \t|  0.019656       \t|\n",
      "on_batch_end              \t|  2.3357e-05     \t|3000           \t|  0.07007        \t|  0.0038118      \t|\n",
      "on_batch_start            \t|  2.1519e-05     \t|3000           \t|  0.064558       \t|  0.0035119      \t|\n",
      "on_train_batch_start      \t|  1.2126e-05     \t|3000           \t|  0.036379       \t|  0.001979       \t|\n",
      "training_step_end         \t|  8.6639e-06     \t|3000           \t|  0.025992       \t|  0.0014139      \t|\n",
      "on_validation_end         \t|  0.010562       \t|2              \t|  0.021123       \t|  0.0011491      \t|\n",
      "on_validation_start       \t|  0.0080924      \t|2              \t|  0.016185       \t|  0.00088044     \t|\n",
      "on_train_start            \t|  0.014335       \t|1              \t|  0.014335       \t|  0.00077982     \t|\n",
      "on_validation_batch_start \t|  1.4157e-05     \t|502            \t|  0.0071071      \t|  0.00038662     \t|\n",
      "validation_step_end       \t|  9.0814e-06     \t|502            \t|  0.0045589      \t|  0.000248       \t|\n",
      "on_before_zero_grad       \t|  2.4032e-05     \t|188            \t|  0.0045181      \t|  0.00024578     \t|\n",
      "on_after_backward         \t|  1.7087e-05     \t|188            \t|  0.0032124      \t|  0.00017475     \t|\n",
      "on_epoch_start            \t|  0.0013144      \t|1              \t|  0.0013144      \t|  7.1501e-05     \t|\n",
      "on_train_end              \t|  0.00046824     \t|1              \t|  0.00046824     \t|  2.5472e-05     \t|\n",
      "on_validation_epoch_end   \t|  3.5848e-05     \t|2              \t|  7.1696e-05     \t|  3.9002e-06     \t|\n",
      "on_validation_epoch_start \t|  1.6329e-05     \t|2              \t|  3.2657e-05     \t|  1.7765e-06     \t|\n",
      "on_fit_start              \t|  2.0822e-05     \t|1              \t|  2.0822e-05     \t|  1.1327e-06     \t|\n",
      "on_epoch_end              \t|  2.0265e-05     \t|1              \t|  2.0265e-05     \t|  1.1024e-06     \t|\n",
      "on_train_epoch_end        \t|  1.0974e-05     \t|1              \t|  1.0974e-05     \t|  5.9698e-07     \t|\n",
      "on_train_epoch_start      \t|  1.0455e-05     \t|1              \t|  1.0455e-05     \t|  5.6875e-07     \t|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-desperate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "egyptian-benefit",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "trainer.test()\n",
    "\n",
    "trainer.save_checkpoint(\"model1.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-intersection",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scenic-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_checkpoints = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "# pre_model = LitModel.load_from_checkpoint(\n",
    "#     checkpoint_path=best_checkpoints).to(\"cuda\")\n",
    "\n",
    "# pre_model.eval()\n",
    "# pre_model.freeze()\n",
    "\n",
    "# transforms = A.Compose([\n",
    "#     A.CenterCrop(img_size, img_size, p=1.),\n",
    "#     A.Resize(img_size, img_size),\n",
    "#     A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                 std=[0.229, 0.224, 0.225],\n",
    "#                 max_pixel_value=255.0,\n",
    "#                 p=1.0),\n",
    "#     ToTensorV2(p=1.0),\n",
    "# ],\n",
    "#     p=1.)\n",
    "\n",
    "# test_img = transforms(image=cv2.imread(\n",
    "#     \"/media/hdd/Datasets/asl/asl_alphabet_test/asl_alphabet_test/C_test.jpg\"))\n",
    "\n",
    "# y_hat = pre_model(test_img[\"image\"].unsqueeze(0).to(\"cuda\"))\n",
    "\n",
    "# label_map\n",
    "\n",
    "# label_map[int(torch.argmax(y_hat, dim=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-tucson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-manitoba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-stress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-butterfly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-suggestion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-second",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-valuable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-helen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
